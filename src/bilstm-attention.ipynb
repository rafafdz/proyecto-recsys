{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7686,"status":"ok","timestamp":1670543172742,"user":{"displayName":"BIANCA SOFIA DEL SOLAR MEDRANO","userId":"05102954798221977640"},"user_tz":180},"id":"KKsWH45hIUgi"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","dtype = torch.FloatTensor"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":567,"status":"ok","timestamp":1670543192061,"user":{"displayName":"BIANCA SOFIA DEL SOLAR MEDRANO","userId":"05102954798221977640"},"user_tz":180},"id":"m0dKjzbxIiEV"},"outputs":[],"source":["# Bi-LSTM(Attention) Parameters\n","embedding_dim = 2\n","n_hidden = 5 # number of hidden units in one cell\n","num_classes = 2  # 0 or 1"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1670543207725,"user":{"displayName":"BIANCA SOFIA DEL SOLAR MEDRANO","userId":"05102954798221977640"},"user_tz":180},"id":"ju9FmL4_ImJz","outputId":"75beada7-cac4-4357-e59c-d6456e8933b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([6, 3])\n"]}],"source":["# 3 words sentences (=sequence_length is 3)\n","sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n","labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n","\n","word_list = \" \".join(sentences).split()\n","word_list = list(set(word_list))\n","word_dict = {w: i for i, w in enumerate(word_list)}\n","vocab_size = len(word_dict)\n","\n","inputs = []\n","for sen in sentences:\n","    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n","\n","targets = []\n","for out in labels:\n","    targets.append(out) # To using Torch Softmax Loss function\n","\n","input_batch = Variable(torch.LongTensor(inputs))\n","target_batch = Variable(torch.LongTensor(targets))\n","\n","\n","print(input_batch.size())"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"elapsed":17458,"status":"ok","timestamp":1670543530267,"user":{"displayName":"BIANCA SOFIA DEL SOLAR MEDRANO","userId":"05102954798221977640"},"user_tz":180},"id":"nO82yXL3JwL_","outputId":"79c10952-dad2-48b0-fa59-1f9704b425a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([6, 3])\n","torch.Size([6, 3, 2])\n"]},{"ename":"TypeError","evalue":"cannot unpack non-iterable NoneType object","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5000\u001b[39m):\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m     output, attention \u001b[39m=\u001b[39m model(input_batch)\n\u001b[1;32m     43\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, target_batch)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"]}],"source":["class BiLSTM_Attention(nn.Module):\n","    def __init__(self):\n","        super(BiLSTM_Attention, self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n","        self.out = nn.Linear(n_hidden * 2, num_classes)\n","\n","    # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n","    def attention_net(self, lstm_output, final_state):\n","        hidden = final_state.view(-1, n_hidden * 2, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n","        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n","        soft_attn_weights = F.softmax(attn_weights, 1)\n","        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n","        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n","        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n","\n","    def forward(self, X):\n","        print(X.size())\n","        input = self.embedding(X) # input : [batch_size, len_seq, embedding_dim]\n","        input = input.permute(1, 0, 2) # input : [len_seq, batch_size, embedding_dim]\n","\n","        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n","        cell_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n","\n","        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n","        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n","        output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n","        attn_output, attention = self.attention_net(output, final_hidden_state)\n","        return self.out(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]\n","\n","model = BiLSTM_Attention()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training\n","for epoch in range(5000):\n","    optimizer.zero_grad()\n","    output, attention = model(input_batch)\n","    loss = criterion(output, target_batch)\n","    if (epoch + 1) % 1000 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","# Test\n","test_text = 'sorry hate you'\n","tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n","test_batch = Variable(torch.LongTensor(tests))\n","\n","# Predict\n","predict, _ = model(test_batch)\n","predict = predict.data.max(1, keepdim=True)[1]\n","if predict[0][0] == 0:\n","    print(test_text,\"is Bad Mean...\")\n","else:\n","    print(test_text,\"is Good Mean!!\")\n","    \n","fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n","ax = fig.add_subplot(1, 1, 1)\n","ax.matshow(attention, cmap='viridis')\n","ax.set_xticklabels(['']+['first_word', 'second_word', 'third_word'], fontdict={'fontsize': 14}, rotation=90)\n","ax.set_yticklabels(['']+['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'], fontdict={'fontsize': 14})\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPN5YyK8iSxX5/EOpbsB50c","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"venv"}},"nbformat":4,"nbformat_minor":0}
